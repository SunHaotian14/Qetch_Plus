{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on Different Time-series Similarity Measures 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of loaded samples per class: [100, 100, 100, 100, 100, 100, 100, 100]\n",
      "Original data: 8 datasets\n",
      "Sketch data: 8 datasets\n"
     ]
    }
   ],
   "source": [
    "# load image and libraries\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from scipy import signal\n",
    "from sklearn import preprocessing\n",
    "\n",
    "root_path = './processed_datasets/'\n",
    "datasets = ['has', 'sp', 'fp', 'rb', 'sd', 'sr', 'hasb', 'ihas']\n",
    "\n",
    "# load ground truth\n",
    "ori_data_X = []\n",
    "ori_data_y = []\n",
    "sketch_X = []\n",
    "for dataset in datasets:\n",
    "    file_name = root_path + 'original_' + dataset  \n",
    "    ori_data_X.append(np.load(file_name + '_X' + '.npy'))\n",
    "    ori_data_y.append(np.load(file_name + '_y' + '.npy'))\n",
    "    file_name = root_path + 'sketch_' + dataset + '.npy'\n",
    "    sketch_X.append(np.load(file_name, allow_pickle=True)[:100])\n",
    "print(f\"number of loaded samples per class: {[len(x) for x in sketch_X]}\")\n",
    "print(f\"Original data: {len(ori_data_X)} datasets\")\n",
    "print(f\"Sketch data: {len(sketch_X)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(ori_series, clip_series, measure):\n",
    "    \"\"\"\n",
    "    Compute the similarities of the original and the clipped series using sliding window\n",
    "    input: original time series, clipped series, similarity measure function\n",
    "    output: similarity_distribution, matching result, i.e., starting and ending points\n",
    "    \"\"\"\n",
    "    ori_len = ori_series.shape[0]\n",
    "    clip_len = clip_series.shape[0]\n",
    "    if ori_len < clip_len:\n",
    "        return None\n",
    "\n",
    "    # compute the similarity between the original and the clipped series\n",
    "    dist = []\n",
    "    # compute the similarity between the original and the clipped series using sliding window\n",
    "    for i in range(ori_len - clip_len + 1):\n",
    "        dist.append(measure(ori_series[i:i+clip_len], clip_series))\n",
    "    # find the maximum similarity and the corresponding starting and ending points\n",
    "    min_idx = np.argmin(dist)\n",
    "    return dist, [min_idx, min_idx + clip_len - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_exp(ori_data_X, ori_data_y, sketch_X, measure):\n",
    "    results = []\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        original = ori_data_X[i]\n",
    "        label = ori_data_y[i]\n",
    "        dummy_record = []\n",
    "        for sample in sketch_X[i]:\n",
    "            clip =  signal.resample(sample, label[1]-label[0]+1)\n",
    "            sim_dist, pred_loc = sliding_window(original, clip, measure)\n",
    "            dummy_record.append([sim_dist, pred_loc])\n",
    "        results.append(dummy_record)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1.1: Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x,y):\n",
    "    t = preprocessing.Normalizer()\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    y = np.expand_dims(y, axis=0)\n",
    "    return np.linalg.norm(t.transform(x) - t.transform(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32mc:\\Georgia Tech\\Fall 2022\\Special Topics - Human in the Loop\\Project\\Qetch_Plus\\Crowd-Study Data\\Experiment1.ipynb Cell 11\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> <a href='vscode-notebook-cell:/c%3A/Georgia%20Tech/Fall%202022/Special%20Topics%20-%20Human%20in%20the%20Loop/Project/Qetch_Plus/Crowd-Study%20Data/Experiment1.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001B[0m results_eu \u001B[39m=\u001B[39m pointwise_exp(ori_data_X, ori_data_y, sketch_X, euclidean_distance)\n",
      "\u001B[1;32mc:\\Georgia Tech\\Fall 2022\\Special Topics - Human in the Loop\\Project\\Qetch_Plus\\Crowd-Study Data\\Experiment1.ipynb Cell 11\u001B[0m in \u001B[0;36mpointwise_exp\u001B[1;34m(ori_data_X, ori_data_y, sketch_X, measure)\u001B[0m\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Georgia%20Tech/Fall%202022/Special%20Topics%20-%20Human%20in%20the%20Loop/Project/Qetch_Plus/Crowd-Study%20Data/Experiment1.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001B[0m \u001B[39mfor\u001B[39;00m sample \u001B[39min\u001B[39;00m sketch_X[i]:\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Georgia%20Tech/Fall%202022/Special%20Topics%20-%20Human%20in%20the%20Loop/Project/Qetch_Plus/Crowd-Study%20Data/Experiment1.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001B[0m     clip \u001B[39m=\u001B[39m  signal\u001B[39m.\u001B[39mresample(sample, label[\u001B[39m1\u001B[39m]\u001B[39m-\u001B[39mlabel[\u001B[39m0\u001B[39m]\u001B[39m+\u001B[39m\u001B[39m1\u001B[39m)\n\u001B[1;32m----> <a href='vscode-notebook-cell:/c%3A/Georgia%20Tech/Fall%202022/Special%20Topics%20-%20Human%20in%20the%20Loop/Project/Qetch_Plus/Crowd-Study%20Data/Experiment1.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001B[0m     sim_dist, pred_loc \u001B[39m=\u001B[39m sliding_window(original, clip, measure)\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Georgia%20Tech/Fall%202022/Special%20Topics%20-%20Human%20in%20the%20Loop/Project/Qetch_Plus/Crowd-Study%20Data/Experiment1.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001B[0m     dummy_record\u001B[39m.\u001B[39mappend([sim_dist, pred_loc])\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Georgia%20Tech/Fall%202022/Special%20Topics%20-%20Human%20in%20the%20Loop/Project/Qetch_Plus/Crowd-Study%20Data/Experiment1.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001B[0m results\u001B[39m.\u001B[39mappend(dummy_record)\n",
      "\u001B[1;32mc:\\Georgia Tech\\Fall 2022\\Special Topics - Human in the Loop\\Project\\Qetch_Plus\\Crowd-Study Data\\Experiment1.ipynb Cell 11\u001B[0m in \u001B[0;36msliding_window\u001B[1;34m(ori_series, clip_series, measure)\u001B[0m\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Georgia%20Tech/Fall%202022/Special%20Topics%20-%20Human%20in%20the%20Loop/Project/Qetch_Plus/Crowd-Study%20Data/Experiment1.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001B[0m \u001B[39m# compute the similarity between the original and the clipped series using sliding window\u001B[39;00m\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Georgia%20Tech/Fall%202022/Special%20Topics%20-%20Human%20in%20the%20Loop/Project/Qetch_Plus/Crowd-Study%20Data/Experiment1.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001B[0m \u001B[39mfor\u001B[39;00m i \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(ori_len \u001B[39m-\u001B[39m clip_len \u001B[39m+\u001B[39m \u001B[39m1\u001B[39m):\n\u001B[1;32m---> <a href='vscode-notebook-cell:/c%3A/Georgia%20Tech/Fall%202022/Special%20Topics%20-%20Human%20in%20the%20Loop/Project/Qetch_Plus/Crowd-Study%20Data/Experiment1.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001B[0m     dist\u001B[39m.\u001B[39mappend(measure(ori_series[i:i\u001B[39m+\u001B[39;49mclip_len], clip_series))\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Georgia%20Tech/Fall%202022/Special%20Topics%20-%20Human%20in%20the%20Loop/Project/Qetch_Plus/Crowd-Study%20Data/Experiment1.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001B[0m \u001B[39m# find the maximum similarity and the corresponding starting and ending points\u001B[39;00m\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Georgia%20Tech/Fall%202022/Special%20Topics%20-%20Human%20in%20the%20Loop/Project/Qetch_Plus/Crowd-Study%20Data/Experiment1.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001B[0m min_idx \u001B[39m=\u001B[39m np\u001B[39m.\u001B[39margmin(dist)\n",
      "\u001B[1;32mc:\\Georgia Tech\\Fall 2022\\Special Topics - Human in the Loop\\Project\\Qetch_Plus\\Crowd-Study Data\\Experiment1.ipynb Cell 11\u001B[0m in \u001B[0;36meuclidean_distance\u001B[1;34m(x, y)\u001B[0m\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Georgia%20Tech/Fall%202022/Special%20Topics%20-%20Human%20in%20the%20Loop/Project/Qetch_Plus/Crowd-Study%20Data/Experiment1.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001B[0m x \u001B[39m=\u001B[39m np\u001B[39m.\u001B[39mexpand_dims(x, axis\u001B[39m=\u001B[39m\u001B[39m0\u001B[39m)\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Georgia%20Tech/Fall%202022/Special%20Topics%20-%20Human%20in%20the%20Loop/Project/Qetch_Plus/Crowd-Study%20Data/Experiment1.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001B[0m y \u001B[39m=\u001B[39m np\u001B[39m.\u001B[39mexpand_dims(y, axis\u001B[39m=\u001B[39m\u001B[39m0\u001B[39m)\n\u001B[1;32m----> <a href='vscode-notebook-cell:/c%3A/Georgia%20Tech/Fall%202022/Special%20Topics%20-%20Human%20in%20the%20Loop/Project/Qetch_Plus/Crowd-Study%20Data/Experiment1.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001B[0m \u001B[39mreturn\u001B[39;00m np\u001B[39m.\u001B[39mlinalg\u001B[39m.\u001B[39mnorm(t\u001B[39m.\u001B[39mtransform(x) \u001B[39m-\u001B[39m t\u001B[39m.\u001B[39;49mtransform(y))\n",
      "File \u001B[1;32mc:\\Users\\vishn\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1955\u001B[0m, in \u001B[0;36mNormalizer.transform\u001B[1;34m(self, X, copy)\u001B[0m\n\u001B[0;32m   1953\u001B[0m copy \u001B[39m=\u001B[39m copy \u001B[39mif\u001B[39;00m copy \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39melse\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mcopy\n\u001B[0;32m   1954\u001B[0m X \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_validate_data(X, accept_sparse\u001B[39m=\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mcsr\u001B[39m\u001B[39m\"\u001B[39m, reset\u001B[39m=\u001B[39m\u001B[39mFalse\u001B[39;00m)\n\u001B[1;32m-> 1955\u001B[0m \u001B[39mreturn\u001B[39;00m normalize(X, norm\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mnorm, axis\u001B[39m=\u001B[39;49m\u001B[39m1\u001B[39;49m, copy\u001B[39m=\u001B[39;49mcopy)\n",
      "File \u001B[1;32mc:\\Users\\vishn\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1792\u001B[0m, in \u001B[0;36mnormalize\u001B[1;34m(X, norm, axis, copy, return_norm)\u001B[0m\n\u001B[0;32m   1789\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m   1790\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\u001B[39m\"\u001B[39m\u001B[39m'\u001B[39m\u001B[39m%d\u001B[39;00m\u001B[39m'\u001B[39m\u001B[39m is not a supported axis\u001B[39m\u001B[39m\"\u001B[39m \u001B[39m%\u001B[39m axis)\n\u001B[1;32m-> 1792\u001B[0m X \u001B[39m=\u001B[39m check_array(\n\u001B[0;32m   1793\u001B[0m     X,\n\u001B[0;32m   1794\u001B[0m     accept_sparse\u001B[39m=\u001B[39;49msparse_format,\n\u001B[0;32m   1795\u001B[0m     copy\u001B[39m=\u001B[39;49mcopy,\n\u001B[0;32m   1796\u001B[0m     estimator\u001B[39m=\u001B[39;49m\u001B[39m\"\u001B[39;49m\u001B[39mthe normalize function\u001B[39;49m\u001B[39m\"\u001B[39;49m,\n\u001B[0;32m   1797\u001B[0m     dtype\u001B[39m=\u001B[39;49mFLOAT_DTYPES,\n\u001B[0;32m   1798\u001B[0m )\n\u001B[0;32m   1799\u001B[0m \u001B[39mif\u001B[39;00m axis \u001B[39m==\u001B[39m \u001B[39m0\u001B[39m:\n\u001B[0;32m   1800\u001B[0m     X \u001B[39m=\u001B[39m X\u001B[39m.\u001B[39mT\n",
      "File \u001B[1;32mc:\\Users\\vishn\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:820\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001B[0m\n\u001B[0;32m    813\u001B[0m     \u001B[39mif\u001B[39;00m n_features \u001B[39m<\u001B[39m ensure_min_features:\n\u001B[0;32m    814\u001B[0m         \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\n\u001B[0;32m    815\u001B[0m             \u001B[39m\"\u001B[39m\u001B[39mFound array with \u001B[39m\u001B[39m%d\u001B[39;00m\u001B[39m feature(s) (shape=\u001B[39m\u001B[39m%s\u001B[39;00m\u001B[39m) while\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[0;32m    816\u001B[0m             \u001B[39m\"\u001B[39m\u001B[39m a minimum of \u001B[39m\u001B[39m%d\u001B[39;00m\u001B[39m is required\u001B[39m\u001B[39m%s\u001B[39;00m\u001B[39m.\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[0;32m    817\u001B[0m             \u001B[39m%\u001B[39m (n_features, array\u001B[39m.\u001B[39mshape, ensure_min_features, context)\n\u001B[0;32m    818\u001B[0m         )\n\u001B[1;32m--> 820\u001B[0m \u001B[39mif\u001B[39;00m copy \u001B[39mand\u001B[39;00m np\u001B[39m.\u001B[39;49mmay_share_memory(array, array_orig):\n\u001B[0;32m    821\u001B[0m     array \u001B[39m=\u001B[39m np\u001B[39m.\u001B[39marray(array, dtype\u001B[39m=\u001B[39mdtype, order\u001B[39m=\u001B[39morder)\n\u001B[0;32m    823\u001B[0m \u001B[39mreturn\u001B[39;00m array\n",
      "File \u001B[1;32m<__array_function__ internals>:5\u001B[0m, in \u001B[0;36mmay_share_memory\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "results_eu = pointwise_exp(ori_data_X, ori_data_y, sketch_X, euclidean_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anlysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results_eu)):\n",
    "    print(f\"Dataset: {datasets[i]}\")\n",
    "    print(f\"Number of samples: {len(results_eu[i])}\")\n",
    "    print(f\"Average distance: {np.mean([np.max(x[0]) for x in results_eu[i]])}\")\n",
    "    print(f\"Average location error w.r.t. segment length: {np.mean([np.abs(x[1][0] - ori_data_y[i][0])/ (ori_data_y[i][1]-ori_data_y[i][0]+1)*100 for x in results_eu[i]])}%\")\n",
    "print(f'Average distance: {np.mean([np.mean([np.max(x[0]) for x in results_eu[i]]) for i in range(len(results_eu))])}')\n",
    "print('--------------------Overall:------------------------------------')\n",
    "print(f'Average location error w.r.t. segment length: {np.mean([np.mean([np.abs(x[1][0] - ori_data_y[i][0])/ (ori_data_y[i][1]-ori_data_y[i][0]+1)*100 for x in results_eu[i]]) for i in range(len(results_eu))])}%')\n",
    "print(f'Average location error w.r.t. total length: {np.mean([np.mean([np.abs(x[1][0] - ori_data_y[i][0])/ ori_data_X[i].shape[0]*100 for x in results_eu[i]]) for i in range(len(results_eu))])}%')\n",
    "\n",
    "for i in range(len(results_eu)):\n",
    "    plt.figure()\n",
    "    plt.title(f\"Dataset: {datasets[i]}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Similarity\")\n",
    "    plt.plot(results_eu[i][0][0])\n",
    "    plt.plot(results_eu[i][0][1], [0.5, 0.5], marker='*', ls='none')\n",
    "    plt.plot(ori_data_y[i], [0.5, 0.5], marker='o', color='r', ls='none')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smoothing\n",
    "import pandas as pd\n",
    "def smoother(series,smoothing):\n",
    "    series_df = pd.DataFrame(series,columns=['Data'])\n",
    "    return series_df.ewm(smoothing).mean().to_numpy() \n",
    "# test = np.array([1, 2, 3, 4,2,5,2,232,323,23,2,3,23,2,3])\n",
    "# op = smoother(test,0.5)\n",
    "# print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qetch Algorithm -- inclomplete --\n",
    "def width(series):\n",
    "    # Should return width of series -  Size of a 1D array is the same as the length.\n",
    "    return series.size\n",
    "    \n",
    "def height(series):\n",
    "    #Finds Height of time series based difference in max and minimum values.\n",
    "    h = np.max(series) - np.min(series) \n",
    "    return h\n",
    "\n",
    "\n",
    "def heightGlobal(series):\n",
    "    hmax = 0\n",
    "    hmin = 999\n",
    "    for i in series:\n",
    "        hmax = max(np.max(i),hmax)\n",
    "        hmin = min(np.min(i),hmin)\n",
    "\n",
    "    h = hmax - hmin\n",
    "    return h\n",
    "\n",
    "def widthGlobal(series):\n",
    "    return series.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_Correcter(split_arr,h_threshold):\n",
    "    # Checks if the height is less than 1% of total height and mergers small segments.\n",
    "    corr_split = []\n",
    "    p = 0\n",
    "    buff = []\n",
    "    split_at = []\n",
    "    counter = 0\n",
    "    for i in split_arr:\n",
    "        if(len(i)==1 or (height(i)<h_threshold)):\n",
    "            buff.append(i)\n",
    "        else:\n",
    "            if(len(buff)>0):\n",
    "                buff.append(i)\n",
    "                temp = np.concatenate(buff)\n",
    "                corr_split.append(temp)\n",
    "\n",
    "                split_at.append(counter) #Starting position of segment is noted\n",
    "                counter+=temp.size\n",
    "                buff = []\n",
    "            else:\n",
    "                split_at.append(counter) #Starting position of segment is noted\n",
    "                counter+=i.size\n",
    "                corr_split.append(i)\n",
    "\n",
    "    split_at.append(counter) # Adding the end position of the last segment\n",
    "\n",
    "    # print(\"Final\",corr_split[:5],\"Number of segments:\",len(corr_split))\n",
    "    return corr_split,len(corr_split),split_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_based_derivative(series):\n",
    "\n",
    "    h_threshold = 0.01 * height(series)\n",
    "    diff_arr = np.diff(series)\n",
    "    sign_arr = np.sign(diff_arr)\n",
    "    p = 0\n",
    "    split_indices = []\n",
    "    split_at = []\n",
    "    for i in range(0,len(sign_arr)):\n",
    "        if(i==0):\n",
    "            p = sign_arr[i]\n",
    "        else:\n",
    "            if((sign_arr[i] == 0) or (sign_arr[i]==1)):\n",
    "                if(p==-1):\n",
    "                    split_indices.append(i)\n",
    "                    p = sign_arr[i]\n",
    "            elif((sign_arr[i] == -1) and ((p==1) or (p==0))):\n",
    "                split_indices.append(i)\n",
    "                p = sign_arr[i]\n",
    "                \n",
    "    # print(series[:10],diff_arr[:10])\n",
    "    split_arr = np.split(series, split_indices, axis=0)\n",
    "    # print(len(split_arr))\n",
    "    # print(len(series))\n",
    "    # print(series[:10])\n",
    "    # print(diff_arr[:10])\n",
    "    # print(sign_arr[:10])\n",
    "    # print(split_indices)\n",
    "    # print(split_arr[:3])\n",
    "\n",
    "    #print(\"Before Split Correcter\")\n",
    "    corrected_split,k,split_at = Split_Correcter(split_arr,h_threshold)\n",
    "    return corrected_split,k,split_at\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LDE(sketch_split,Candidate_split,Gx,Gy):\n",
    "    Rx = width(Candidate_split)/(Gx * width(sketch_split))    \n",
    "    Ry = height(Candidate_split)/(Gy * height(sketch_split))  \n",
    "    return (np.log(Rx)**2)+(np.log(Ry)**2)\n",
    "\n",
    "# from scipy.spatial.distance import cityblock\n",
    "# print(cityblock(x1, x2))\n",
    "\n",
    "def get_ShapeError(sketch_split,candidate_split,Gy):\n",
    "\n",
    "    Ni = min(candidate_split.size,sketch_split.size)\n",
    "    Sum_of_Shape = 0 \n",
    "    #print(\"Candidate split\", candidate_split,\"sketch split\", sketch_split, \"NI\",Ni,\"size:\",sketch_split.size,candidate_split.size)\n",
    "\n",
    "\n",
    "    #print(\"NI\",Ni,\"sketch and candidate size:\",sketch_split.size,candidate_split.size)\n",
    "\n",
    "    resampled_sketch_split = signal.resample(sketch_split,Ni)\n",
    "    resampled_candidate_split = signal.resample(candidate_split,Ni)\n",
    "\n",
    "    Ry = height(resampled_candidate_split)/(Gy * height(resampled_sketch_split)) \n",
    "\n",
    "\n",
    "    for i in range(0,Ni):\n",
    "        Sum_of_Shape += abs(((Gy*Ry*resampled_sketch_split[i]) - resampled_candidate_split[i])/height(candidate_split))        \n",
    "\n",
    "    return Sum_of_Shape/Ni\n",
    "    \n",
    "def calculateDistance(Sketch, Candidate,k):\n",
    "    Sketch = np.array(Sketch)\n",
    "    Candidate = np.array(Candidate)\n",
    "\n",
    "    #print(\"calculateDistance: Length of the Candidate and Sketch Segment\",Candidate.size,Sketch.size,k)\n",
    "\n",
    "    # Calculating Global non uniform Scaling factors\n",
    "    Gx = widthGlobal(Candidate)/widthGlobal(Sketch)\n",
    "    Gy = heightGlobal(Candidate)/heightGlobal(Sketch)\n",
    "    # Calculating Local distortion and shape errors\n",
    "    LDE = 0\n",
    "    SE = 0\n",
    "    for i in range(0,k-1):\n",
    "        LDE += get_LDE(Sketch[i],Candidate[i],Gx,Gy)\n",
    "        SE += get_ShapeError(Sketch[i],Candidate[i],Gy)\n",
    "\n",
    "    # Calculating total error\n",
    "    Dist = LDE + SE\n",
    "    return Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Precision\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_interpreter_precision(results,curve):\n",
    "\n",
    "    op = []\n",
    "    sorted_results = sorted(results, key=lambda x: x[0])\n",
    "\n",
    "    j = 0\n",
    "    if(sorted_results[0][0] == 999):\n",
    "        return [0,0,0]\n",
    "    while(j<5):\n",
    "        if(sorted_results[j][3]==curve):\n",
    "            if(j==0):\n",
    "                return [1,1,1]\n",
    "            elif(j<3):\n",
    "                return [0,1,1]\n",
    "            elif(j<5):\n",
    "                return [0,0,1]\n",
    "    return [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def qetch_plus_Precision(ori_data_X, ori_data_y, sketch_X, smooth_val_stepsize,curve):\n",
    "    z = 0\n",
    "    prec_output = []\n",
    "    while(z<=100):\n",
    "        results = []\n",
    "        for i, dataset in enumerate(datasets):\n",
    "            original = ori_data_X[i]\n",
    "            ResultDistanceObject = []\n",
    "            Candidate_split_at = []\n",
    "            Sketch_split_at = []\n",
    "            #Segments Loaded Data into T segments\n",
    "            # print(\"Before splitting Original\")\n",
    "\n",
    "            split_original,T,Candidate_split_at = split_based_derivative(original)\n",
    "\n",
    "            # print(\" T value\",T)\n",
    "\n",
    "            testing_sketch = sketch_X[curve][z]\n",
    "\n",
    "            # print(\"Before splitting sketch\")\n",
    "            #Assuming input is converted to a modifiable bezier curve\n",
    "            split_sketch,k,Sketch_split_at = split_based_derivative(testing_sketch)\n",
    "\n",
    "            # print(\"T value and k Value are: \",T,k)\n",
    "            if(T<k):\n",
    "                ResultDistanceObject.append([999,[0,0],smooth_value,i])\n",
    "                print(\"not possible\") #Need to address case where this happens -> Smoothen Sketches with too much K\n",
    "                continue\n",
    "            itr = 0\n",
    "            while(itr<=T-k):\n",
    "                candidate_segments = split_original[itr:k+itr]\n",
    "                query_segment = split_sketch\n",
    "                itr+=1\n",
    "                smooth_value = 0.1\n",
    "                while(smooth_value < 1):\n",
    "                    smoothed_candidate_segments = []\n",
    "                    DistanceObject = []\n",
    "                    for l in range(0,len(candidate_segments)):\n",
    "                        smoothed_candidate_segments.append(smoother(candidate_segments[l],smooth_value))\n",
    "\n",
    "                    #print(\"Smoothed Candidate segments: \",len(smoothed_candidate_segments))\n",
    "                    # for l in range(0,len(smoothed_candidate_segments)):\n",
    "                    distance = calculateDistance(query_segment,smoothed_candidate_segments,k)\n",
    "                    #Add the starting and ending position Identified\n",
    "                    start_pos = Candidate_split_at[itr]\n",
    "                    end_pos = Candidate_split_at[itr+1]\n",
    "                    DistanceObject.append([distance,[start_pos,end_pos],smooth_value,i])\n",
    "                    smooth_value += smooth_val_stepsize\n",
    "\n",
    "                ResultDistanceObject.append(min(DistanceObject, key = lambda sublist: sublist[0])) # Will Contain a list of T-k minimum distances\n",
    "\n",
    "                    # print(len(l))\n",
    "                # print(len(smoothed_candidate_segments[0]))\n",
    "\n",
    "            results.append(min(ResultDistanceObject, key = lambda sublist: sublist[0]))\n",
    "        prec_output.append(result_interpreter_precision(results,curve)) #Should contain 8 best minimum distances\n",
    "        print(\"--- Completed a sketch --- \")\n",
    "        z+=20\n",
    "    s1,s3,s5 = 0,0,0\n",
    "    for q in prec_output:\n",
    "        s1+=q[0]\n",
    "        s3+=q[1]\n",
    "        s5 += q[2]\n",
    "    l = len(prec_output)\n",
    "    s1 = s1/len\n",
    "    s3 = s3/len\n",
    "    s5 = s5/len\n",
    "    precision_op = [s1,s3,s5]\n",
    "\n",
    "    return precision_op"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def qetch_plus_tester_precision(curve):\n",
    "    smooth_val_stepsize = 0.05\n",
    "    # for i in range(len(datasets)):\n",
    "    return qetch_plus_Precision(ori_data_X, ori_data_y, sketch_X, smooth_val_stepsize,curve)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not possible\n"
     ]
    }
   ],
   "source": [
    "#datasets = ['has', 'sp', 'fp', 'rb', 'sd', 'sr', 'hasb', 'ihas']\n",
    "precision = []\n",
    "for i in range(0,8):\n",
    "    precision.append(qetch_plus_tester_precision(i))\n",
    "    print(\"Completed single type\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# d['Average location error (%)'].append(np.mean([np.abs(x[1][0] - ori_data_y[i][0])/ ori_data_X[i].shape[0]*100 for x in results[i]]))\n",
    "def result_interpreter_accuracy(results,ori_data_y):\n",
    "\n",
    "    for x in results:\n",
    "\n",
    "    average_distance = np.mean()\n",
    "\n",
    "    return [accuracy,average_distance]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def qetch_plus_accuracy(ori_data_X, ori_data_y, sketch_X, smooth_val_stepsize,curve):\n",
    "    z = 0\n",
    "    results = []\n",
    "    while(z<=100):\n",
    "\n",
    "        ResultDistanceObject = []\n",
    "        original = ori_data_X[i]\n",
    "        Candidate_split_at = []\n",
    "        Sketch_split_at = []\n",
    "        #Segments Loaded Data into T segments\n",
    "        # print(\"Before splitting Original\")\n",
    "        split_original,T,Candidate_split_at = split_based_derivative(original)\n",
    "        # print(\" T value\",T)\n",
    "        testing_sketch = sketch_X[curve][z]\n",
    "        # print(\"Before splitting sketch\")\n",
    "        #Assuming input is converted to a modifiable bezier curve\n",
    "        split_sketch,k,Sketch_split_at = split_based_derivative(testing_sketch)\n",
    "        # print(\"T value and k Value are: \",T,k)\n",
    "        if(T<k):\n",
    "            ResultDistanceObject.append([999,[0,0],smooth_value,i])\n",
    "            print(\"not possible\") #Need to address case where this happens -> Smoothen Sketches with too much K\n",
    "            continue\n",
    "        itr = 0\n",
    "        while(itr<=T-k):\n",
    "            candidate_segments = split_original[itr:k+itr]\n",
    "            query_segment = split_sketch\n",
    "            itr+=1\n",
    "            smooth_value = 0.1\n",
    "            while(smooth_value < 1):\n",
    "                smoothed_candidate_segments = []\n",
    "                DistanceObject = []\n",
    "                for l in range(0,len(candidate_segments)):\n",
    "                    smoothed_candidate_segments.append(smoother(candidate_segments[l],smooth_value))\n",
    "                distance = calculateDistance(query_segment,smoothed_candidate_segments,k)\n",
    "                #Add the starting and ending position Identified\n",
    "                start_pos = Candidate_split_at[itr]\n",
    "                end_pos = Candidate_split_at[itr+1]\n",
    "                DistanceObject.append([distance,[start_pos,end_pos],smooth_value,i])\n",
    "                smooth_value += smooth_val_stepsize\n",
    "            ResultDistanceObject.append(min(DistanceObject, key = lambda sublist: sublist[0])) # Will Contain a list of T-k minimum distances\n",
    "\n",
    "        results.append(min(ResultDistanceObject, key = lambda sublist: sublist[0])) # Should contain 100 minimum distances.\n",
    "        z+=20\n",
    "    return result_interpreter_accuracy(results,ori_data_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def qetch_plus_tester_accuracy(curve):\n",
    "    smooth_val_stepsize = 0.05\n",
    "    # for i in range(len(datasets)):\n",
    "    return qetch_plus_accuracy(ori_data_X, ori_data_y, sketch_X, smooth_val_stepsize,curve)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#datasets = ['has', 'sp', 'fp', 'rb', 'sd', 'sr', 'hasb', 'ihas']\n",
    "accuracy = []\n",
    "for i in range(0,8):\n",
    "    accuracy.append(qetch_plus_tester_precision(i))\n",
    "    print(\"Accuracy is:\",accuracy[i][0],\"Average Distance is\",accuracy[i][1])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ed1e60d53ec687cd08561468826879ac387fa6cb426728f3989bdf7ac79679c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
