{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on Different Time-series Similarity Measures 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of loaded samples per class: [100, 100, 100, 100, 100, 100, 100, 100]\n",
      "Original data: 8 datasets\n",
      "Sketch data: 8 datasets\n"
     ]
    }
   ],
   "source": [
    "# load image and libraries\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from scipy import signal\n",
    "from sklearn import preprocessing\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "root_path = './processed_datasets/'\n",
    "datasets = ['has', 'sp', 'fp', 'rb', 'sd', 'sr', 'hasb', 'ihas']\n",
    "\n",
    "# load ground trutht\n",
    "ori_data_X = []\n",
    "ori_data_y = []\n",
    "sketch_X = []\n",
    "for dataset in datasets:\n",
    "    file_name = root_path + 'original_' + dataset  \n",
    "    ori_data_X.append(np.load(file_name + '_X' + '.npy'))\n",
    "    ori_data_y.append(np.load(file_name + '_y' + '.npy'))\n",
    "    file_name = root_path + 'sketch_' + dataset + '.npy'\n",
    "    sketch_X.append(np.load(file_name, allow_pickle=True)[:100])\n",
    "print(f\"number of loaded samples per class: {[len(x) for x in sketch_X]}\")\n",
    "print(f\"Original data: {len(ori_data_X)} datasets\")\n",
    "print(f\"Sketch data: {len(sketch_X)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smoothing\n",
    "import pandas as pd\n",
    "def smoother(series,smoothing):\n",
    "    series_df = pd.DataFrame(series,columns=['Data'])\n",
    "    return series_df.ewm(alpha = np.exp(- 3 * smoothing)).mean().to_numpy() \n",
    "# test = np.array([1, 2, 3, 4,2,5,2,232,323,23,2,3,23,2,3])\n",
    "# op = smoother(test,0.5)\n",
    "# print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qetch Algorithm -- inclomplete --\n",
    "def width(series):\n",
    "    # Should return width of series -  Size of a 1D array is the same as the length.\n",
    "    return series.size\n",
    "    \n",
    "def height(series):\n",
    "    #Finds Height of time series based difference in max and minimum values.\n",
    "    h = np.max(series) - np.min(series) \n",
    "    return h\n",
    "\n",
    "\n",
    "def heightGlobal(series):\n",
    "    hmax = 0\n",
    "    hmin = 999\n",
    "    for i in series:\n",
    "        hmax = max(np.max(i),hmax)\n",
    "        hmin = min(np.min(i),hmin)\n",
    "\n",
    "    h = hmax - hmin\n",
    "    return h\n",
    "\n",
    "def widthGlobal(series):\n",
    "    return series.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_Correcter(split_arr,h_threshold):\n",
    "    # Checks if the height is less than 1% of total height and mergers small segments.\n",
    "    corr_split = []\n",
    "    p = 0\n",
    "    buff = []\n",
    "    split_at = []\n",
    "    counter = 0\n",
    "    for i in split_arr:\n",
    "        if(len(i)==1 or (height(i)<h_threshold)):\n",
    "            buff.append(i)\n",
    "        else:\n",
    "            if(len(buff)>0):\n",
    "                buff.append(i)\n",
    "                temp = np.concatenate(buff)\n",
    "                corr_split.append(temp)\n",
    "\n",
    "                split_at.append(counter) #Starting position of segment is noted\n",
    "                counter+=temp.size\n",
    "                buff = []\n",
    "            else:\n",
    "                split_at.append(counter) #Starting position of segment is noted\n",
    "                counter+=i.size\n",
    "                corr_split.append(i)\n",
    "\n",
    "    split_at.append(counter) # Adding the end position of the last segment\n",
    "\n",
    "    # print(\"Final\",corr_split[:5],\"Number of segments:\",len(corr_split))\n",
    "    return corr_split,len(corr_split),split_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_based_derivative(series,c = False):\n",
    "\n",
    "    if(c):\n",
    "        series = series.reshape(series.size)\n",
    "\n",
    "    h_threshold = 0.01 * height(series)\n",
    "    diff_arr = np.diff(series)\n",
    "    sign_arr = np.sign(diff_arr)\n",
    "\n",
    "    # print(\"THE SIZE IS:\",series.shape[0])\n",
    "    # print(\"The actual Arr:\",series[:50])\n",
    "    # print(\"Diff array\",diff_arr[:10])\n",
    "    # print(\"Sign array\",sign_arr[:10])\n",
    "    p = 0\n",
    "    split_indices = []\n",
    "    split_at = []\n",
    "    for i in range(0,len(sign_arr)):\n",
    "        if(i==0):\n",
    "            p = sign_arr[i]\n",
    "        else:\n",
    "            if((sign_arr[i] == 0) or (sign_arr[i]==1)):\n",
    "                if(p==-1):\n",
    "                    split_indices.append(i)\n",
    "                    p = sign_arr[i]\n",
    "            elif((sign_arr[i] == -1) and ((p==1) or (p==0))):\n",
    "                split_indices.append(i)\n",
    "                p = sign_arr[i]\n",
    "                \n",
    "    # print(series[:10],diff_arr[:10])\n",
    "    split_arr = np.split(series, split_indices, axis=0)\n",
    "    # print(len(split_arr))\n",
    "    # print(len(series))\n",
    "    # print(series[:10])\n",
    "    # print(diff_arr[:10])\n",
    "    # print(sign_arr[:10])\n",
    "    # print(split_indices)\n",
    "    # print(split_arr[:3])\n",
    "\n",
    "    #print(\"Before Split Correcter\")\n",
    "    corrected_split,k,split_at = Split_Correcter(split_arr,h_threshold)\n",
    "    return corrected_split,k,split_at\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LDE(sketch_split,Candidate_split,Gx,Gy):\n",
    "    Rx = width(Candidate_split)/(Gx * width(sketch_split))    \n",
    "    Ry = height(Candidate_split)/(Gy * height(sketch_split))  \n",
    "    return (np.log(Rx)**2)+(np.log(Ry)**2)\n",
    "\n",
    "# from scipy.spatial.distance import cityblock\n",
    "# print(cityblock(x1, x2))\n",
    "\n",
    "def get_ShapeError(sketch_split,candidate_split,Gy):\n",
    "\n",
    "    Ni = min(candidate_split.size,sketch_split.size)\n",
    "    Sum_of_Shape = 0 \n",
    "    #print(\"Candidate split\", candidate_split,\"sketch split\", sketch_split, \"NI\",Ni,\"size:\",sketch_split.size,candidate_split.size)\n",
    "\n",
    "\n",
    "    #print(\"NI\",Ni,\"sketch and candidate size:\",sketch_split.size,candidate_split.size)\n",
    "\n",
    "    resampled_sketch_split = signal.resample(sketch_split,Ni)\n",
    "    resampled_candidate_split = signal.resample(candidate_split,Ni)\n",
    "\n",
    "    Ry = height(resampled_candidate_split)/(Gy * height(resampled_sketch_split)) \n",
    "\n",
    "\n",
    "    for i in range(0,Ni):\n",
    "        Sum_of_Shape += abs(((Gy*Ry*resampled_sketch_split[i]) - resampled_candidate_split[i])/height(candidate_split))        \n",
    "\n",
    "    return Sum_of_Shape/Ni\n",
    "    \n",
    "def calculateDistance(Sketch, Candidate,k):\n",
    "    Sketch = np.array(Sketch)\n",
    "    Candidate = np.array(Candidate)\n",
    "\n",
    "    #print(\"calculateDistance: Length of the Candidate and Sketch Segment\",Candidate.size,Sketch.size,k)\n",
    "\n",
    "    # Calculating Global non uniform Scaling factors\n",
    "    Gx = widthGlobal(Candidate)/widthGlobal(Sketch)\n",
    "    Gy = heightGlobal(Candidate)/heightGlobal(Sketch)\n",
    "    # Calculating Local distortion and shape errors\n",
    "    LDE = 0\n",
    "    SE = 0\n",
    "    for i in range(0,k-1):\n",
    "        LDE += get_LDE(Sketch[i],Candidate[i],Gx,Gy)\n",
    "        SE += get_ShapeError(Sketch[i],Candidate[i],Gy)\n",
    "\n",
    "    # Calculating total error\n",
    "    Dist = LDE + SE\n",
    "    return Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_interpreter_precision(results,curve):\n",
    "\n",
    "    op = []\n",
    "    sorted_results = sorted(results, key=lambda x: x[0])\n",
    "\n",
    "    j = 0\n",
    "    if(sorted_results[0][0] == 999):\n",
    "        return [0,0,0]\n",
    "    while(j<5):\n",
    "        if(sorted_results[j][3]==curve):\n",
    "            if(j==0):\n",
    "                return [1,1,1]\n",
    "            elif(j<3):\n",
    "                return [0,1,1]\n",
    "            elif(j<5):\n",
    "                return [0,0,1]\n",
    "        j+=1\n",
    "    return [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def qetch_plus_Precision(ori_data_X, ori_data_y, sketch_X, smooth_val_stepsize,curve):\n",
    "    z = 0\n",
    "    prec_output = []\n",
    "    while(z<100):\n",
    "        results = []\n",
    "        for i, dataset in enumerate(datasets):\n",
    "            ResultDistanceObject = []\n",
    "            original = ori_data_X[i]\n",
    "            testing_sketch = sketch_X[curve][z]\n",
    "            #Smoothing by factor of smooth_val_stepsize\n",
    "            smoothed_candidate_list = []\n",
    "            smooth_value_list = []\n",
    "            smooth_value = 0\n",
    "            while(smooth_value < 1):\n",
    "                smoothed_candidate_list.append(smoother(original,smooth_value))\n",
    "                smooth_value_list.append(smooth_value)\n",
    "                smooth_value += smooth_val_stepsize\n",
    "            \n",
    "            Sketch_split_at = []\n",
    "            split_sketch,k,Sketch_split_at = split_based_derivative(testing_sketch)\n",
    "            for a in range(len(smoothed_candidate_list)):\n",
    "                Candidate_split_at = []\n",
    "                DistanceObject = []\n",
    "                #Segments Loaded Data into T segments\n",
    "                split_original,T,Candidate_split_at = split_based_derivative(smoothed_candidate_list[a],True)\n",
    "               # print(\"T value and k Value are: \",T,k)\n",
    "                if(T<k):\n",
    "                    ResultDistanceObject.append([999,[0,0],smooth_value_list[a],i])\n",
    "                    print(\"not possible\") #Need to address case where this happens -> Smoothen Sketches with too much K\n",
    "                    continue\n",
    "                itr = 0\n",
    "\n",
    "                while(itr<=T-k):\n",
    "                    candidate_segments = split_original[itr:k+itr]\n",
    "                    query_segment = split_sketch\n",
    "                    itr+=1\n",
    "                    distance = calculateDistance(query_segment,candidate_segments,k)\n",
    "                    #Add the starting and ending position Identified\n",
    "                    start_pos = Candidate_split_at[itr]\n",
    "                    end_pos = Candidate_split_at[itr+1]\n",
    "                    DistanceObject.append([distance,[start_pos,end_pos], [a],i])\n",
    "                # print(\"Distance object \",DistanceObject)\n",
    "                ResultDistanceObject.append(min(DistanceObject, key = lambda sublist: sublist[0])) # Will Contain a list of T-k minimum distances\n",
    "            results.append(min(ResultDistanceObject, key = lambda sublist: sublist[0]))\n",
    "            print(\"--- Completed a sketch --- \")\n",
    "        prec_output.append(result_interpreter_precision(results,curve)) #Should contain 8 best minimum distances\n",
    "        print(\"--- Completed a set of sketches --- \")\n",
    "        z+=1\n",
    "    print(\"Prec op:\",prec_output)\n",
    "    s1,s3,s5 = 0,0,0\n",
    "    for q in prec_output:\n",
    "        s1+=q[0]\n",
    "        s3+=q[1]\n",
    "        s5 += q[2]\n",
    "    l = len(prec_output)\n",
    "    s1 = s1/l\n",
    "    s3 = s3/l\n",
    "    s5 = s5/l\n",
    "    precision_op = [s1,s3,s5]\n",
    "\n",
    "    return precision_op\n",
    "\n",
    "#datasets = ['has', 'sp', 'fp', 'rb', 'sd', 'sr', 'hasb', 'ihas']\n",
    "precision = []\n",
    "smooth_val_stepsize = 0.1\n",
    "for i in range(0,8):\n",
    "    precision.append(qetch_plus_Precision(ori_data_X, ori_data_y, sketch_X, smooth_val_stepsize,i))\n",
    "    print(\"Completed single type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02, 0.09, 0.24], [0.22, 0.74, 0.88], [0.34, 0.95, 0.98], [0.22, 0.7, 0.76], [0.3, 0.87, 0.96], [0.85, 0.98, 1.0], [0.09, 0.22, 0.46], [0.04, 0.35, 0.8]]\n"
     ]
    }
   ],
   "source": [
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# d['Average location error (%)'].append(np.mean([np.abs(x[1][0] - ori_data_y[i][0])/ ori_data_X[i].shape[0]*100 for x in results[i]]))\n",
    "def result_interpreter_error(results,ori_data_y,curve,length_of_original):\n",
    "\n",
    "    error = np.mean([np.abs(x[1][0] - ori_data_y[curve][0])/length_of_original*100 for x in results])\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc65ad247fca4235a325c22636dce596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "The set is:  rb\n",
      "The results looks like:  [[41.222473427311144, [18, 80], 0.051000000000000004, 3], [1.819340438607734, [167, 296], 0.001, 3]]\n",
      "Truth value is:  [118 246]\n",
      "The error is :  9.398550724637682\n"
     ]
    }
   ],
   "source": [
    "def qetch_plus_accuracy(ori_data_X, ori_data_y, sketch_X, smooth_val_stepsize,curve):\n",
    "    results = []\n",
    "\n",
    "    original = ori_data_X[curve]\n",
    "    length_of_original = original.shape[0]\n",
    "\n",
    "    #Smoothing by factor of smooth_val_stepsize\n",
    "    smoothed_candidate_list = []\n",
    "    smooth_value_list = []\n",
    "    smooth_value = 0.001\n",
    "    while(smooth_value < 1):\n",
    "        smoothed_candidate_list.append(smoother(original,smooth_value))\n",
    "        smooth_value_list.append(smooth_value)\n",
    "        smooth_value += smooth_val_stepsize\n",
    "    # print(\"smoothed candidate sketches \",len(smoothed_candidate_list),smoothed_candidate_list[0][:10])\n",
    "\n",
    "    smoothed_sketches = [smoother(sk, 0.3) for sk in sketch_X[curve]]\n",
    "    # smoothed_sketches = sketch_X[curve]\n",
    "    for z in trange(100):\n",
    "        ResultDistanceObject = []\n",
    "        # testing_sketch = sketch_X[curve][z]\n",
    "        testing_sketch = smoothed_sketches[z]\n",
    "        Sketch_split_at = []\n",
    "        split_sketch,k,Sketch_split_at = split_based_derivative(testing_sketch, True)\n",
    "\n",
    "        for a in range(0,len(smoothed_candidate_list)):\n",
    "            Candidate_split_at = []\n",
    "            #Segments Loaded Data into T segments\n",
    "\n",
    "            split_original,T,Candidate_split_at = split_based_derivative(smoothed_candidate_list[a],True)\n",
    "\n",
    "            if(T<k):\n",
    "                ResultDistanceObject.append([999,[0,0],smooth_value_list[a],curve])\n",
    "                print(\"not possible\") #Need to address case where this happens -> Smoothen Sketches with too much K\n",
    "                continue\n",
    "            \n",
    "            itr = 0\n",
    "            DistanceObject = []\n",
    "            while(itr<=T-k):\n",
    "                distance = calculateDistance(split_sketch, split_original[itr:k+itr], k)\n",
    "                #Add the starting and ending position Identified\n",
    "                start_pos = Candidate_split_at[itr]\n",
    "                end_pos = Candidate_split_at[itr+1]\n",
    "                DistanceObject.append([distance,[start_pos,end_pos],smooth_value_list[a],curve])\n",
    "                itr+=1\n",
    "            ResultDistanceObject.append(min(DistanceObject, key = lambda sublist: sublist[0])) # Will Contain a list of T-k minimum distances\n",
    "        results.append(min(ResultDistanceObject, key = lambda sublist: sublist[0])) # Should contain 100 minimum distances.\n",
    "    print(\"The set is: \",datasets[curve])\n",
    "    print(\"The results looks like: \", results[:2])\n",
    "    print(\"Truth value is: \",ori_data_y[curve])\n",
    "    op = result_interpreter_error(results,ori_data_y,curve,length_of_original)\n",
    "    print(\"The error is : \",op)\n",
    "    return op\n",
    "\n",
    "smooth_val_stepsize = 0.05\n",
    "op = qetch_plus_accuracy(ori_data_X, ori_data_y, sketch_X, smooth_val_stepsize,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9498f0cb74a4917a6ac48fb2183425f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "The set is:  has\n",
      "The results looks like:  [[110.58951470837243, [71, 88], 0.501, 0], [26.420141116658357, [66, 128], 0.7010000000000002, 0]]\n",
      "Truth value is:  [107 243]\n",
      "The error is :  10.659420289855072\n",
      "error is: 10.659420289855072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9712ccf95754a4e8175e470f00d720c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set is:  sp\n",
      "The results looks like:  [[35.55649195111231, [240, 247], 0.201, 1], [34.1380806117899, [247, 256], 0.301, 1]]\n",
      "Truth value is:  [247 359]\n",
      "The error is :  6.951690821256038\n",
      "error is: 6.951690821256038\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15648466ede24d24b8369d060ee34414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set is:  fp\n",
      "The results looks like:  [[29.30728126820935, [202, 214], 0.40099999999999997, 2], [42.229366431985966, [202, 214], 0.40099999999999997, 2]]\n",
      "Truth value is:  [131 229]\n",
      "The error is :  11.594202898550723\n",
      "error is: 11.594202898550723\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff922f0953e240c4a0fc7565ee2dae29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "The set is:  rb\n",
      "The results looks like:  [[41.222473427311144, [18, 80], 0.051000000000000004, 3], [1.819340438607734, [167, 296], 0.001, 3]]\n",
      "Truth value is:  [118 246]\n",
      "The error is :  9.398550724637682\n",
      "error is: 9.398550724637682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4623623236428ea8b4e5bb107b14dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set is:  sd\n",
      "The results looks like:  [[123.60882381573785, [146, 152], 0.15100000000000002, 4], [7.021008326938006, [150, 181], 0.001, 4]]\n",
      "Truth value is:  [162 211]\n",
      "The error is :  13.396135265700487\n",
      "error is: 13.396135265700487\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472e4efaf36c4b769f1cf6e5f06a315a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set is:  sr\n",
      "The results looks like:  [[26.104482141513504, [60, 74], 0.40099999999999997, 5], [24.46603291874604, [60, 74], 0.40099999999999997, 5]]\n",
      "Truth value is:  [170 230]\n",
      "The error is :  23.23188405797102\n",
      "error is: 23.23188405797102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd951e3257f4426aa216ba461f93c109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "The set is:  hasb\n",
      "The results looks like:  [[114.36111154606371, [201, 206], 0.201, 6], [33.448091795036234, [0, 28], 0.9010000000000004, 6]]\n",
      "Truth value is:  [ 80 145]\n",
      "The error is :  23.862318840579714\n",
      "error is: 23.862318840579714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4201362bbc14420ebe5202af0f371130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "not possible\n",
      "The set is:  ihas\n",
      "The results looks like:  [[7.8198711064523, [107, 143], 0.9510000000000004, 7], [30.526326968102573, [165, 169], 0.251, 7]]\n",
      "Truth value is:  [170 254]\n",
      "The error is :  14.551807228915658\n",
      "error is: 14.551807228915658\n"
     ]
    }
   ],
   "source": [
    "#datasets = ['has', 'sp', 'fp', 'rb', 'sd', 'sr', 'hasb', 'ihas']\n",
    "error = []\n",
    "for i in range(0,8):\n",
    "    error.append(qetch_plus_accuracy(ori_data_X, ori_data_y, sketch_X, smooth_val_stepsize,i))\n",
    "    print(\"error is:\",error[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.205751265933301\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7565caab8eb65219391f22c8065d75359b61fe2354d09a736de30f82e6269c7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
